{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:45:07.104582Z\",\"iopub.execute_input\":\"2021-12-21T07:45:07.104959Z\",\"iopub.status.idle\":\"2021-12-21T07:45:10.049377Z\",\"shell.execute_reply.started\":\"2021-12-21T07:45:07.104873Z\",\"shell.execute_reply\":\"2021-12-21T07:45:10.04789Z\"}}\n# basic files\nimport time, os, json, os, math\nfrom collections import Counter,OrderedDict\nfrom typing import Any, Optional\nfrom pprint import pprint\n\nimport pandas as pd\nimport numpy as np\n\nimport fuzzywuzzy as fw\nfrom fuzzywuzzy import fuzz\n\nimport nltk\nfrom nltk.corpus import wordnet\nnltk.download('wordnet')\n\nimport torch\nimport torchtext as tt\nfrom torchtext.data.metrics import bleu_score\n\nfrom tqdm.notebook import tqdm, trange\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:50:44.403236Z\",\"iopub.execute_input\":\"2021-12-21T07:50:44.403586Z\",\"iopub.status.idle\":\"2021-12-21T07:50:44.4123Z\",\"shell.execute_reply.started\":\"2021-12-21T07:50:44.403556Z\",\"shell.execute_reply\":\"2021-12-21T07:50:44.411144Z\"}}\n# loading the infotabs english files \ninfotabs_tables_path = \"../input/xinfotabs/infotabs_tables\"\ninfotabs_tables = os.listdir(infotabs_tables_path)\n\nprint(f\"Number of tables in infotabs: {len(infotabs_tables)}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:50:46.150684Z\",\"iopub.execute_input\":\"2021-12-21T07:50:46.151006Z\",\"iopub.status.idle\":\"2021-12-21T07:50:46.193573Z\",\"shell.execute_reply.started\":\"2021-12-21T07:50:46.150976Z\",\"shell.execute_reply\":\"2021-12-21T07:50:46.192593Z\"}}\n# loading a subset to decrease the load on models\nsubset_path = \"../input/xinfotabs/subsets/subset_300.json\"\nwith open(subset_path, \"r\") as file:\n    subset = json.loads(file.read())\n\nprint(f\"Size of the subset: {100*subset['size']:.2f}% \")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:50:47.295628Z\",\"iopub.execute_input\":\"2021-12-21T07:50:47.29594Z\",\"iopub.status.idle\":\"2021-12-21T07:50:47.336196Z\",\"shell.execute_reply.started\":\"2021-12-21T07:50:47.295911Z\",\"shell.execute_reply\":\"2021-12-21T07:50:47.335369Z\"}}\n# loading the category table\ncat_table_path = \"../input/xinfotabs/table_categories.tsv\"\ncat_table = pd.read_csv(cat_table_path, delimiter='\\t', index_col=0)\ncat_table.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:50:51.112851Z\",\"iopub.execute_input\":\"2021-12-21T07:50:51.113164Z\",\"iopub.status.idle\":\"2021-12-21T07:50:51.12064Z\",\"shell.execute_reply.started\":\"2021-12-21T07:50:51.113135Z\",\"shell.execute_reply\":\"2021-12-21T07:50:51.119403Z\"}}\n# example of taking cateogory from table\ncat_table.loc['T1008'].category\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:50:51.452359Z\",\"iopub.execute_input\":\"2021-12-21T07:50:51.452675Z\",\"iopub.status.idle\":\"2021-12-21T07:50:51.457229Z\",\"shell.execute_reply.started\":\"2021-12-21T07:50:51.452647Z\",\"shell.execute_reply\":\"2021-12-21T07:50:51.456117Z\"}}\n# util to load tables\ndef json_to_dict(path : str) -> dict:\n    with open(path, 'r') as file:\n        result_dict = dict(json.loads(file.read()))\n    return result_dict\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:50:55.205347Z\",\"iopub.execute_input\":\"2021-12-21T07:50:55.205665Z\",\"iopub.status.idle\":\"2021-12-21T07:50:57.868803Z\",\"shell.execute_reply.started\":\"2021-12-21T07:50:55.205635Z\",\"shell.execute_reply\":\"2021-12-21T07:50:57.867946Z\"}}\n# creating category | key | value sentences\nsubset_texts = []\nfor table_code in subset['subset']:\n    # loading the table\n    table_path = infotabs_tables_path + '/' + table_code + '.json'\n    table = json_to_dict(table_path)\n    \n    # getting the category\n    try:\n        category = cat_table.loc[table_code].category\n    except:\n        category = 'None'\n        \n    # getting all the values for each key  \n    for key in table.keys():\n        subset_texts += [category + ' | ' + key + ' | ' + value for value in table[key]]\n        \nsubset_texts = list(set(subset_texts))\nprint(f\"Length of the subset inputs: {len(subset_texts)}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:50:57.871847Z\",\"iopub.execute_input\":\"2021-12-21T07:50:57.87211Z\",\"iopub.status.idle\":\"2021-12-21T07:54:38.507302Z\",\"shell.execute_reply.started\":\"2021-12-21T07:50:57.872083Z\",\"shell.execute_reply\":\"2021-12-21T07:54:38.505358Z\"}}\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\").to(device)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:54:38.509667Z\",\"iopub.execute_input\":\"2021-12-21T07:54:38.51003Z\",\"iopub.status.idle\":\"2021-12-21T07:54:38.517566Z\",\"shell.execute_reply.started\":\"2021-12-21T07:54:38.509993Z\",\"shell.execute_reply\":\"2021-12-21T07:54:38.516669Z\"}}\ndef translate(text : str, src_code : str = \"en\", trg_code : str = \"en\", model = None, tokenizer = None,  device: str = 'cpu') -> str :\n  tokenizer.src_lang = src_code\n  encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n  generated_tokens = model.generate(\n      **encoded,\n      forced_bos_token_id = tokenizer.get_lang_id(trg_code)\n  )\n  res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n  return res[0]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:54:38.519039Z\",\"iopub.execute_input\":\"2021-12-21T07:54:38.519394Z\",\"iopub.status.idle\":\"2021-12-21T07:54:39.671338Z\",\"shell.execute_reply.started\":\"2021-12-21T07:54:38.519357Z\",\"shell.execute_reply\":\"2021-12-21T07:54:39.670369Z\"}}\n# test of translation\ntranslate('Hello.', src_code='en',  trg_code='zh', model = model, tokenizer = tokenizer, device=device)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:54:39.672671Z\",\"iopub.execute_input\":\"2021-12-21T07:54:39.673175Z\",\"iopub.status.idle\":\"2021-12-21T07:54:39.67738Z\",\"shell.execute_reply.started\":\"2021-12-21T07:54:39.673137Z\",\"shell.execute_reply\":\"2021-12-21T07:54:39.676359Z\"}}\n# declare all the langauge codes\nog_lang_code = \"en\"\ntr_lang_code = \"zh\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T07:58:06.614808Z\",\"iopub.execute_input\":\"2021-12-21T07:58:06.615213Z\",\"iopub.status.idle\":\"2021-12-21T10:26:44.233338Z\",\"shell.execute_reply.started\":\"2021-12-21T07:58:06.615178Z\",\"shell.execute_reply\":\"2021-12-21T10:26:44.23229Z\"}}\n# translating all the texts to another language\n\n# since the error of direct splitting is too much, we need to employ another method\n# If it can be split directly, it would be split directly\n# Else, we use '////' and '<PAD>' tokens because they seem to work the best out of tried tokens\n# If nothing works, we'll split the input, translate everything and then combine\n\n\ntr_subset_texts = []\n\n\npunc_1 = '////'\npunc_2 = '<PAD>'\n\nerr_1 = []   # can't be split directly into 3 parts\nerr_2 = []   # can't be split with punc_1 into 3 parts\nerr_3 = []   # can't be split with punc_2 into 3 parts\n\n\nfor i in trange(len(subset_texts)):\n    text = subset_texts[i]\n    \n    \n    tr_text_dir = translate(text, og_lang_code, tr_lang_code, model, tokenizer, device)\n    tr_text_dir_list = tr_text_dir.split('|')\n    \n    if len(tr_text_dir_list) == 3:\n        tr_subset_text.append(tr_text_dir)\n    else:\n        err_1.append(i)\n        tr_text_punc_1 = translate(text.replace(\"|\", punc_1), og_lang_code, tr_lang_code, model, tokenizer, device)\n        tr_text_punc_1_list = tr_text_punc_1.split(punc_1)\n        if len(tr_text_punc_1_list) == 3:\n            tr_subset_texts.append(tr_text_punc_1.replace(punc_1, \"|\"))\n        else: \n            err_2.append(i)\n            tr_text_punc_2 = translate(text.replace(\"|\", punc_2), og_lang_code, tr_lang_code, model, tokenizer, device)\n            tr_text_punc_2_list = tr_text_punc_2.split(punc_2)\n            if len(tr_text_punc_2_list) == 3:\n                tr_subset_texts.append(tr_text_punc_2.replace(punc_2, \"|\"))\n            else:\n                err_3.append(i)\n                cat = text.split(\" | \")[0].strip()\n                key = text.split(\" | \")[1].strip()\n                value = text.split(\" | \")[2].strip()\n                \n                tr_cat = translate(cat, og_lang_code, tr_lang_code, model, tokenizer, device)\n                tr_key = translate(key, og_lang_code, tr_lang_code, model, tokenizer, device)\n                tr_value = translate(value, og_lang_code, tr_lang_code,model, tokenizer, device)\n                \n                tr_text = tr_cat + ' | ' + tr_key + ' | ' + tr_value\n                                       \n                tr_subset_texts.append(tr_text)\n        \n    \nprint(f\"Length of translated texts: {len(tr_subset_texts)}\")\nprint(f\"Errors: { len(err_1) } { len(err_2) } { len(err_3) }\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T10:26:44.237563Z\",\"iopub.execute_input\":\"2021-12-21T10:26:44.237953Z\",\"iopub.status.idle\":\"2021-12-21T10:26:44.298727Z\",\"shell.execute_reply.started\":\"2021-12-21T10:26:44.237914Z\",\"shell.execute_reply\":\"2021-12-21T10:26:44.297711Z\"}}\ntemp_df = pd.DataFrame.from_dict({\"subset_texts\":subset_texts,\n                                  \"tr_subset_texts\":tr_subset_texts\n                                 })\ntemp_df.to_csv(\"./ZH_temp_context_attached_M2M100.csv\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T10:26:44.303446Z\",\"iopub.execute_input\":\"2021-12-21T10:26:44.305493Z\",\"iopub.status.idle\":\"2021-12-21T10:26:44.310682Z\",\"shell.execute_reply.started\":\"2021-12-21T10:26:44.305449Z\",\"shell.execute_reply\":\"2021-12-21T10:26:44.309896Z\"}}\n# temp_df = pd.read_csv(\"../input/xinfotabs/temp/RU_temp_context_attached_M2M100.csv\",index_col=0)\n# subset_texts = list(temp_df['subset_texts'])\n# tr_subset_texts = list(temp_df['tr_subset_texts'])\n\n# print(f\"{ len(subset_texts) }  { len(tr_subset_texts) }\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T10:26:44.314984Z\",\"iopub.execute_input\":\"2021-12-21T10:26:44.317707Z\",\"iopub.status.idle\":\"2021-12-21T10:26:44.32838Z\",\"shell.execute_reply.started\":\"2021-12-21T10:26:44.31766Z\",\"shell.execute_reply\":\"2021-12-21T10:26:44.327358Z\"}}\n# # experiments with tokens for attaching context\n# ind = 2\n# print(subset_texts[ind].replace(\"|\",\"</s>\"))\n# print(translate(subset_texts[ind].replace(\"|\",\"</s>\"), og_lang_code, tr_lang_code, model, tokenizer, device))\n\n# %% [code] {\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2021-12-21T10:26:44.333584Z\",\"iopub.execute_input\":\"2021-12-21T10:26:44.336054Z\",\"iopub.status.idle\":\"2021-12-21T10:26:44.342279Z\",\"shell.execute_reply.started\":\"2021-12-21T10:26:44.336011Z\",\"shell.execute_reply\":\"2021-12-21T10:26:44.341369Z\"}}\n# # running experiments on attached context seperation for 100 random samples\n\n# err_1 = 0\n# err_2 = 0\n# samples = np.random.randint(low = 0, high = len(subset_texts), size = (100, ))\n# print(samples)\n\n# punc = '</s>'\n# punc_sub = '</s>'\n# for i in tqdm(samples):\n    \n#     tr_text = translate(subset_texts[i].replace(\"|\",punc), og_lang_code, tr_lang_code, model, tokenizer, device)\n    \n#     try: \n#         tr_text_list = tr_text.split(punc)\n        \n#         if len(tr_text_list) != 3:\n#             assert len(tr_text_list) == 3\n        \n#         key = tr_text_list[1].strip()\n    \n#     except:\n#         err_1+=1\n        \n#         tr_text = translate(subset_texts[i].replace(\"|\",punc_sub), og_lang_code, tr_lang_code, model, tokenizer, device)\n        \n#         try: \n#             tr_text_list = tr_text.split(punc_sub)\n        \n#             if len(tr_text_list) != 3:\n#                 assert len(tr_text_list) == 3\n        \n#             key = tr_text_list[1].strip()\n            \n#         except:\n#             err_2+=1\n        \n        \n# print(f\"{ len(samples) }  { err_1 } { err_2 } \")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T10:26:44.347185Z\",\"iopub.execute_input\":\"2021-12-21T10:26:44.348563Z\",\"iopub.status.idle\":\"2021-12-21T10:26:44.377949Z\",\"shell.execute_reply.started\":\"2021-12-21T10:26:44.34852Z\",\"shell.execute_reply\":\"2021-12-21T10:26:44.37714Z\"}}\n# having a look at the translations\n\nfor text in tr_subset_texts:\n    text_list = text.split(\" | \")\n    if len(text_list) != 3 :  \n        print(f\" {text} \")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T10:26:44.381576Z\",\"iopub.execute_input\":\"2021-12-21T10:26:44.383442Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.581601Z\",\"shell.execute_reply.started\":\"2021-12-21T10:26:44.383397Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.580714Z\"}}\n# back translating all the text to original language\nbt_subset_texts = []\n\n# since the error of direct splitting is too much, we need to employ another method\n# If it can be split directly, it would be split directly\n# Else, we use '////' and '<PAD>' tokens because they seem to work the best out of tried tokens\n# If nothing works, we'll split the input, translate everything and then combine\n\n\n\npunc_1 = '////'\npunc_2 = '<PAD>'\n\nbt_err_1 = []   # can't be split directly into 3 parts\nbt_err_2 = []   # can't be split with punc_1 into 3 parts\nbt_err_3 = []   # can't be split with punc_2 into 3 parts\n\n\nfor i in trange(len(tr_subset_texts)):\n    text = tr_subset_texts[i]\n    \n    bt_text_dir = translate(text, tr_lang_code, og_lang_code, model, tokenizer, device)\n    bt_text_dir_list = bt_text_dir.split('|')\n    \n    if len(bt_text_dir_list) == 3:\n        bt_subset_text.append(bt_text_dir)\n    else:\n        bt_err_1.append(i)\n        bt_text_punc_1 = translate(text.replace(\"|\", punc_1), tr_lang_code, og_lang_code, model, tokenizer, device)\n        bt_text_punc_1_list = bt_text_punc_1.split(punc_1)\n        if len(bt_text_punc_1_list) == 3:\n            bt_subset_texts.append(bt_text_punc_1.replace(punc_1, \"|\"))\n        else: \n            bt_err_2.append(i)\n            bt_text_punc_2 = translate(text.replace(\"|\", punc_2), tr_lang_code, og_lang_code, model, tokenizer, device)\n            bt_text_punc_2_list = bt_text_punc_2.split(punc_2)\n            if len(bt_text_punc_2_list) == 3:\n                bt_subset_texts.append(bt_text_punc_2.replace(punc_2, \"|\"))\n            else:\n                bt_err_3.append(i)\n                \n                try: \n                    cat = text.split(\" | \")[0].strip()\n                except:\n                    cat = \"none\"\n                \n                try:\n                    key = text.split(\" | \")[1].strip()\n                except:\n                    key = \"none\"\n                \n                try:\n                    value = text.split(\" | \")[2].strip()\n                except:\n                    value = \"none\"\n                \n                bt_cat = translate(cat, tr_lang_code, og_lang_code, model, tokenizer, device)\n                bt_key = translate(key, tr_lang_code, og_lang_code, model, tokenizer, device)\n                bt_value = translate(value, tr_lang_code, og_lang_code,model, tokenizer, device)\n                \n                bt_text = bt_cat + ' | ' + bt_key + ' | ' + bt_value\n                                       \n                bt_subset_texts.append(bt_text)\n        \n    \nprint(f\"Length of translated texts: {len(bt_subset_texts)}\")\nprint(f\"Errors: { len(bt_err_1) } { len(bt_err_2) } { len(bt_err_3) }\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.584392Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.584962Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.59217Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.584922Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.591402Z\"}}\nbt_dict = {\"bt\":bt_subset_texts}\nwith open(\"./bt.json\", \"w\") as file:\n    file.write(json.dumps(bt_dict,))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.593995Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.594347Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.645673Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.594312Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.644787Z\"}}\nbt_texts_df = pd.DataFrame.from_dict({\"original_texts\":subset_texts,\n                                      \"translated_texts\": tr_subset_texts,\n                                      \"backtranslated_texts\":bt_subset_texts\n                                     })\n\nbt_texts_df.to_csv(\"./ZH_context_texts_backtranslation_error_analysis_M2M100_300_v1.csv\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.646891Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.647398Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.655448Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.647359Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.654439Z\"}}\nsubset_keys = [text.split(' | ')[1] for text in subset_texts]\nprint(f\"Length of subset keys: {len(subset_keys)} and example: {subset_keys[1]}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.656919Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.65763Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.669048Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.657552Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.668206Z\"}}\ntr_subset_keys = [text.split('|')[1].strip() for text in tr_subset_texts]\nprint(f\"Length of subset keys: {len(tr_subset_keys)} and example: {tr_subset_keys[1]}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.671453Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.671755Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.682681Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.67172Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.681874Z\"}}\nbt_subset_keys = []\nfor i, text in enumerate(bt_subset_texts):\n    try:\n        bt_key =  text.split('|')[1].strip()\n    except:\n        print(i)\n        bt_key = \"None\"\n    bt_subset_keys.append(bt_key)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.684074Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.684652Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.712493Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.684616Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.711647Z\"}}\n# saving the context keys\nbt_keys_df = pd.DataFrame.from_dict({\"original_keys\":subset_keys,\n                                      \"translated_keys\": tr_subset_keys,\n                                      \"backtranslated_keys\":bt_subset_keys\n                                     })\n\nbt_keys_df.to_csv(\"./ZH_context_keys_backtranslation_error_analysis_M2M100_300_v1.csv\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.714151Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.714429Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.72223Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.714405Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.721226Z\"}}\nsubset_values = [text.split(' | ')[-1] for text in subset_texts]\nprint(f\"Length of subset keys: {len(subset_values)} and example: {subset_values[1]}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.723861Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.724542Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.738359Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.724459Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.737299Z\"}}\ntr_subset_values = [text.split('|')[-1].strip() for text in tr_subset_texts]\nprint(f\"Length of subset keys: {len(tr_subset_values)} and example: {tr_subset_values[1]}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.739789Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.740135Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.753331Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.7401Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.75251Z\"}}\nbt_subset_values = []\nfor i, text in enumerate(bt_subset_texts):\n    try:\n        bt_value =  text.split('|')[-1].strip()\n    except:\n        print(i)\n        bt_value = \"None\"\n    bt_subset_values.append(bt_value)\nprint(f\"Length of subset keys: {len(bt_subset_values)} and example: {bt_subset_values[1]}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.756315Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.756635Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.792704Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.756591Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.791817Z\"}}\n# saving the context keys\nbt_values_df = pd.DataFrame.from_dict({\"original_values\":subset_values,\n                                      \"translated_values\": tr_subset_values,\n                                      \"backtranslated_values\":bt_subset_values\n                                     })\n\nbt_values_df.to_csv(\"./ZH_context_values_backtranslation_error_analysis_M2M100_300_v1.csv\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.794233Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.794634Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.899414Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.794594Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.897767Z\"}}\nlev_dis = []\n\nfor i in range(len(subset_keys)):\n  ratio = fuzz.ratio(str(subset_keys[i]), str(bt_subset_keys[i]))\n  err_ratio = 1 - (ratio/100)\n  dist = err_ratio * (len(str(subset_keys[i])) if len(str(subset_keys[i])) > len(str(bt_subset_keys[i])) else len(str(bt_subset_keys[i])))\n  lev_dis.append(dist)\n    \nbt_keys_df['levenshtein_dist'] = lev_dis\n\ntemp = len(bt_keys_df[bt_keys_df[\"levenshtein_dist\"]<3])\nprint(f\"Small lev dist keys: {temp} \")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.900756Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.901088Z\",\"iopub.status.idle\":\"2021-12-21T12:36:19.945538Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.90105Z\",\"shell.execute_reply\":\"2021-12-21T12:36:19.944543Z\"}}\nlev_dis = []\n\nfor i in range(len(subset_values)):\n  ratio = fuzz.ratio(str(subset_values[i]), str(bt_subset_values[i]))\n  err_ratio = 1 - (ratio/100)\n  dist = err_ratio * (len(str(subset_values[i])) if len(str(subset_values[i])) > len(str(bt_subset_values[i])) else len(str(bt_subset_values[i])))\n  lev_dis.append(dist)\n    \nbt_values_df['levenshtein_dist'] = lev_dis\n\ntemp = len(bt_values_df[bt_values_df[\"levenshtein_dist\"]<3])\nprint(f\"Small lev dist keys: {temp} \")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:19.94692Z\",\"iopub.execute_input\":\"2021-12-21T12:36:19.947285Z\",\"iopub.status.idle\":\"2021-12-21T12:36:30.098358Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:19.947234Z\",\"shell.execute_reply\":\"2021-12-21T12:36:30.097488Z\"}}\nwordnet_similarity_score = []\nfor i in trange(len(subset_keys)):\n  try: \n    syn_og = wordnet.synsets(subset_keys[i])[0]\n  except:\n    wordnet_similarity_score.append(0)\n    continue\n\n  try:\n    syn_bt = wordnet.synsets(bt_subset_keys[i])[0]\n  except:\n    wordnet_similarity_score.append(0)\n    continue\n  \n  ss = wordnet.wup_similarity(syn_og, syn_bt)\n  wordnet_similarity_score.append(ss)\n\nbt_keys_df['wordnet_similarity_score'] = wordnet_similarity_score\n\ntemp = len(bt_keys_df[ bt_keys_df[\"wordnet_similarity_score\"] > 0 ])\nprint(f\"High wordnet sim score: {temp}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:30.099738Z\",\"iopub.execute_input\":\"2021-12-21T12:36:30.100308Z\",\"iopub.status.idle\":\"2021-12-21T12:36:38.347246Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:30.100254Z\",\"shell.execute_reply\":\"2021-12-21T12:36:38.346353Z\"}}\nwordnet_similarity_score = []\nfor i in trange(len(subset_values)):\n  try: \n    syn_og = wordnet.synsets(subset_values[i])[0]\n  except:\n    wordnet_similarity_score.append(0)\n    continue\n\n  try:\n    syn_bt = wordnet.synsets(bt_subset_values[i])[0]\n  except:\n    wordnet_similarity_score.append(0)\n    continue\n  \n  ss = wordnet.wup_similarity(syn_og, syn_bt)\n  wordnet_similarity_score.append(ss)\n\nbt_values_df['wordnet_similarity_score'] = wordnet_similarity_score\n\ntemp = len(bt_values_df[ bt_values_df[\"wordnet_similarity_score\"] > 0 ])\nprint(f\"High wordnet sim score: {temp}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:38.348591Z\",\"iopub.execute_input\":\"2021-12-21T12:36:38.349103Z\",\"iopub.status.idle\":\"2021-12-21T12:36:49.254989Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:38.349063Z\",\"shell.execute_reply\":\"2021-12-21T12:36:49.254041Z\"}}\n!pip install sentence-transformers\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:36:49.259399Z\",\"iopub.execute_input\":\"2021-12-21T12:36:49.259705Z\",\"iopub.status.idle\":\"2021-12-21T12:37:18.363482Z\",\"shell.execute_reply.started\":\"2021-12-21T12:36:49.259675Z\",\"shell.execute_reply\":\"2021-12-21T12:37:18.362621Z\"}}\nfrom sentence_transformers import SentenceTransformer, util\nsentence_model = SentenceTransformer('paraphrase-mpnet-base-v2').to(device)\n\n# %% [code] {\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:37:18.366492Z\",\"iopub.execute_input\":\"2021-12-21T12:37:18.366787Z\",\"iopub.status.idle\":\"2021-12-21T12:48:23.467162Z\",\"shell.execute_reply.started\":\"2021-12-21T12:37:18.366761Z\",\"shell.execute_reply\":\"2021-12-21T12:48:23.466282Z\"}}\nparaphrase_score = []\n\nfor i in trange(len(subset_keys)): \n  og_embed = sentence_model.encode(str(subset_keys[i]))\n  bt_embed = sentence_model.encode(str(bt_subset_keys[i]))\n  cos_sim = util.cos_sim(og_embed, bt_embed)\n  paraphrase_score.append(cos_sim.item())\n\nbt_keys_df['paraphrase_score'] = paraphrase_score\n\ntemp = len(bt_keys_df[bt_keys_df['paraphrase_score'] > 0.5])\nprint(f\"high paraphrase score: {temp}\")\n\n# %% [code] {\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2021-12-21T12:48:23.468648Z\",\"iopub.execute_input\":\"2021-12-21T12:48:23.469202Z\",\"iopub.status.idle\":\"2021-12-21T12:59:31.330239Z\",\"shell.execute_reply.started\":\"2021-12-21T12:48:23.469163Z\",\"shell.execute_reply\":\"2021-12-21T12:59:31.329186Z\"}}\nparaphrase_score = []\n\nfor i in trange(len(subset_values)): \n  og_embed = sentence_model.encode(str(subset_values[i]))\n  bt_embed = sentence_model.encode(str(bt_subset_values[i]))\n  cos_sim = util.cos_sim(og_embed, bt_embed)\n  paraphrase_score.append(cos_sim.item())\n\nbt_values_df['paraphrase_score'] = paraphrase_score\n\ntemp = len(bt_values_df[bt_values_df['paraphrase_score'] > 0.5])\nprint(f\"high paraphrase score: {temp}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-12-21T13:05:44.256744Z\",\"iopub.execute_input\":\"2021-12-21T13:05:44.257093Z\",\"iopub.status.idle\":\"2021-12-21T13:05:44.342647Z\",\"shell.execute_reply.started\":\"2021-12-21T13:05:44.257053Z\",\"shell.execute_reply\":\"2021-12-21T13:05:44.341772Z\"}}\nbt_keys_df.to_csv(\"./ZH_context_keys_backtranslation_error_analysis_M2M100_300_v2.csv\")\nbt_values_df.to_csv(\"./ZH_context_values_backtranslation_error_analysis_M2M100_300_v2.csv\")\n\n# %% [code]\n","metadata":{"_uuid":"56579819-c8c8-41df-a2eb-f6019e15e34c","_cell_guid":"af046879-f59f-4cd1-8c4b-d735ff9079c6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}